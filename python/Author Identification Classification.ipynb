{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    " Using Naive Bayes and Logistic Regression to perform authorship identification. \n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer (an automatic tokenizer similar to the NLTK one we used in the lab), so you may notice the odd non-sentence here and there. Our objective is to accurately identify the author of the sentences in the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Description</h2>\n",
    "\n",
    "There are only three columns in the CSV file: \n",
    "- \"**id**\": The id of the sample. You should not use this column.\n",
    "- \"**text**\": The sentence that you will need to build your model with.\n",
    "- \"**author**\": The target you need to predict.\n",
    "  1. _EAP_: Edgar Allan Poe\n",
    "  2. _HPL_: HP Lovecraft\n",
    "  3. _MWS_: Mary Shelley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 1: Explore the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Load the training data `train.csv` into a Dataframe named `data`.\n",
    "\n",
    "Import the necessary packages if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# TODO:\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Separate the dataframe into the data (the \"text\" column) and the targets (the \"author\" column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y= df['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "  \n",
    "Explore the training data \n",
    "\n",
    "For example, check the distribution of different authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>5635</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>6044</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  text\n",
       "author            \n",
       "EAP     7900  7900\n",
       "HPL     5635  5635\n",
       "MWS     6044  6044"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('author').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 2: Split data into training and testing\n",
    "\n",
    "splitting the data into 80% for training and 20% for testing.\n",
    "\n",
    "Call the resulting training and test data as `X_train` and `X_test`; the resulting training and test targets as `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do the 80-20 train/test splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 3: Preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Write a function to tokenize the text into words. We assume each text is one single sentence, so there is no need to do sentence splitting here.\n",
    "\n",
    "> Hint: use NLTK's `word_tokenize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import nltk\n",
    "\n",
    "def tokenize_words(text):\n",
    "    # TODO tokenize the text into words\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Remove stopwords from the text.\n",
    "\n",
    "> Hint: use stopwords from NLTK to do the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # TODO: return a new list of words with any stopwords removed\n",
    "     return [word.lower() for word in words if word.lower() not in en_stopwords]   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Combine the first two steps to perform end-to-end preprocessing for a given text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "def preprocess(text):\n",
    "    words = tokenize_words(text) # tokenize\n",
    "    words = remove_stopwords(words) # remove stopwords\n",
    "    \n",
    "    # combine the words together into one single string, \n",
    "    # such that the down-stream vectorizer can work with\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Test `preprocess` function in the previous step by running through some text and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset contains text works fiction written spooky authors public domain : edgar allan poe , hp lovecraft mary shelley .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence= 'The dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley.'\n",
    "preprocess(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Use `preprocess()` function defined in Step 3 to process each text in `X_train`. Do the same for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "X_train = [preprocess(x) for x in X_train]\n",
    "\n",
    "# TODO: preprocess X_test in a similar way\n",
    "X_test = [preprocess(x) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 3: Vectorize the text data\n",
    "\n",
    "After the preprocessing, we need to vectorize, i.e., transform the words into actual numeric numbers that the model can digest. \n",
    "\n",
    "For text data, [Tf-idf vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is a good choice, as it is able to represent each word using its TF-IDF weights.\n",
    "\n",
    "> `TF-IDF` weights , which is an empirically solid measurement of word importance in text.\n",
    "\n",
    "> For more info, look at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize an Tf-Idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2) \n",
    "# by setting `min_df=2`, we will ignore any words with less than 2 occurrences in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Fit the Tf-IDF vectorizer on our **training data**.\n",
    "\n",
    "> Hint: use the `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# TODO: use the vectorizer defined above to fit and transform the training data `X_train`\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Transform the **test data** using the vectorizer fitted in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# TODO: use the vectorizer fitted in the previous step to transform the test data `X_test`\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 4: Train a Naive Bayes classifier\n",
    "\n",
    "Here, we will use Sklearn's [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) to train our authorship classifier.\n",
    "\n",
    "> For more info: see http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize a MultinomialNB classifier object and call it `nb`. \n",
    "\n",
    "Read its documentation (http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) to figure out if you want to use any non-default values when initializing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# TODO: initialize a MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Fit the classifier `nb` using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "# TODO: fit the NB classifier on training data\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Use the trained `nb` classifier to classify the texts in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# TODO: predict test data\n",
    "\n",
    "y_preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Write a function to evaluate your predictions against the truth. This is a classification problem, so we need to use classification metrics here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "from sklearn.metrics.classification import classification_report, accuracy_score\n",
    "\n",
    "def report_performance(y_preds, y_test):\n",
    "    # TODO: implement this reporting function\n",
    "    acc = accuracy_score(y_preds, y_test)\n",
    "    \n",
    "    print(f'accuracy: {acc}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Call `report_performance` function to report the performance of the `nb` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.81511746680286\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "# TODO: report the performance of your `nb` classifier\n",
    "report_performance(y_preds, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 5: Train a Logistic Regression classifier\n",
    "\n",
    "In addition to the Naive Bayes classifier we just experimented with, we can also try the Logistic Regression model \n",
    "\n",
    "Again, we can use Sklearn's [Logistic Regression model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize a LogisticRegression object and call it `lr`. Train `lr` with the training data.\n",
    "\n",
    "> Hint: think about whether you need to set the `class_weight` argument when initializing the `LogisticRegression` object, base on your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# TODO: Initialize a LogisticRegression object and call it `lr`. Train `lr` with the training data.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Use the trained `lr` classifier to classify the texts in the test data. Report the performance of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.804902962206333\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "# TODO: Use the trained lr classifier to classify the texts in the test data. \n",
    "# Report the performance of this classifier.\n",
    "y_preds = lr.predict(X_test)\n",
    "\n",
    "report_performance(y_preds, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
