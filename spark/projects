#### Retail Store Recommendation system
hadoop fs -copyFromLocal casestudy2 /user/justycx201703

val retailData = sc.textFile("casestudy2/retail_2013.csv")

retailData.count
// Get the number of fields in data
val numfields = retailData.take(1)(0).split(',').length

//Get distinct products
val products = retailData.map(line=>line.split(',')(4)).distinct.cache()
products.foreach(println)

// Create a map from products to their index. This is required to create an int field from products
val productMap = products.zipWithIndex.collect.toMap

//Get the product and custid fields and count
case class cData(items: Int, custid: Int, cnt: Double)

val countData = retailData.map {x=>
    val buffer = x.split(',')
    cData(productMap(buffer(4)).toInt,buffer(6).toInt,1.0)
}.toDF

// Create a temp view and aggregate the counts for each item and customer
countData.createOrReplaceTempView("Rating")
val gData = spark.sql("select items,custid, sum(cnt) as cnt from Rating group by items,custid").cache()

// check few rows 
gData.show(5)

// Specify hyper parameters for the model:
val iterations = 5
val lambda = 0.01
val alpha = 1.0

// Create an alternate least squares model

val als = new ALS().setMaxIter(iterations).setRegParam(lambda).setUserCol("custid").setItemCol("items").setRatingCol("cnt").setImplicitPrefs(true).setAlpha(alpha)

// Fit the model to training data
val model = als.fit(gData)


// create a reverse map for products
val productReverse = productMap.map(_.swap)

//check recommendation for a user:
def suggest_5products(user: Int) = {
    val userDF = productMap.values.map(x=>cData(x.toInt,user,0.0)).toSeq.toDS()
   val predict = model.transform(userDF)
   val recommendations = predict.select("items").orderBy(desc("prediction")).head(5)
    recommendations.map(x=>productReverse(x.getInt(0))).foreach(println)
   
}

// test the above function for different users

scala> suggest_5products(43124)
Kitchen & Dining
Boots
Handbags
Platinum jewelry
Head Lights

scala> suggest_5products(99970)
Perfumes
Kitchen furnishings
Chocolates & Candies
Vehicle Power adapters
Outdoor furniture

*****************************************************************************************************
BANK TRANSACTION PROCESSING
*****************************************************************************************************
hadoop fs -copyFromLocal casestudy6 /user/justycx201703

import org.apache.spark.HashPartitioner
//Load the data
val tfile1=sc.textFile("casestudy6/transactions_08.csv")
val tfile2=sc.textFile("casestudy6/transactions_09.csv")

tfile1.count
tfile2.count

tfile1.take(1)
tfile2.take(1)

//create case class for values
case class tran(source : String, amount : Int)
