## QN 31 Removing the text from a file and saving it to a folder
Load the context text file:

val content = sc.textFile("Spark2/Content.txt")
val remove = sc.textFile("Spark2/Remove.txt")
val removeRDD = remove.flatMap(x => x.split(",")).map(word => word.trim)
val bRemove= sc.broadcast(removeRDD.collect().toList)
val words = content.flatMap(line => line.split(" "))
val filtered = words.filter{case(word) => !bRemove.value.contains(word)}
val pairRDD = filtered.map(word => (word,1))
val wordCount = pairRDD.reduceByKey(_+_)
wordCount.saveAsTextFile("Spark2/result.txt")

## QN 32 Filter words in a text
Load 3 text files in a folder to spark

val content = sc.textFile("Spark3/sparkdir1/file1.txt,Spark3/sparkdir2/file2.txt,Spark3/sparkdir3/file3.txt")
content.take(5).foreach(println)

val flatContent = content.flatMap(word => word.split(" "))
flatContent.take(10).foreach(println)

val trimmedContent = flatContent.map(word => word.trim)
val removeRDD = sc.parallelize(List("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of"))
val filtered = trimmedContent.subtract(removeRDD)
val pairRDD = filtered.map(word => (word,1))
val wordCount = pairRDD.reduceByKey(_+_)
val swapped = wordCount.map(item => item.swap)
val sortedOutput = swapped.sortByKey(false)
sortedOutput.saveAsTextFile("Spark3/result")
import org.apache.hadoop.io.compress.GzipCodec
sortedOutput.saveAsTextFile("Spark3/compressedresult",classOf[GzipCodec])

## QN 33 Join tables in scala
Load tables into hdfs
hadoop fs -copyFromLocal spark5 /user/justycx201703

val name = sc.textFile("spark5/EmployeeName.csv")
val namePairRDD = name.map(x =>(x.split(",")(0),x.split(",")(1)))

val salary= sc.textFile("spark5/EmployeeSalary.csv")
val salaryPairRDD = salary.map(x =>(x.split(",")(0),x.split(",")(1)))

val joined = namePairRDD.join(salaryPairRDD)
val keyRemoved = joined.values
val swapped = keyRemoved.map(item => item.swap)
val grpByKey = swapped.groupByKey.collect()
val rddByKey = grpByKey.map{case(k,v) => k->sc.makeRDD(v.toSeq)}
rddByKey.foreach{case(k,rdd) => rdd.saveAsTextFile("spark5/Employee"+k)}

## QN 34 ceating an rdd, removing headers, and filtering words
Load the data into hdfs
hadoop fs -copyFromLocal spark6 /user/justycx201703

val csv = sc.textFile("spark6/user.csv")
val headerAndRows = csv.map(line => line.split(",").map(_.trim))
val header = headerAndRows.first
val data = headerAndRows.filter(_(0)!= header(0))
val maps = data.map(splits => header.zip(splits).toMap)
#Filter out the user myself
val result = maps.filter(map => map("id") != "myself")
result.saveAsTextFile("spark6/result.txt")

## QN 35 sort the file by name and save into a folder
Load the data into hdfs
hadoop fs -copyFromLocal spark7 /user/justycx201703

val name = sc.textFile("spark7/EmployeeName.csv")
val namePairRDD = name.map(x => (x.split(",")(0), x.split(",")(1)))
val swapped = namePairRDD.map(item => item.swap)
val sortedOutput = swapped.sortByKey()
val swappedBack = sortedOutput.map(item => item.swap)
## save the file as one textfile
swappedBack.repartition(1).saveAsTextFile("spark7/result.txt")

## QN 36 Grouping and save into a folder
hadoop fs -copyFromLocal spark8 /user/justycx201703

val name = sc.textFile("spark8/data.csv")
val namePairRDD = name.map(x => (x.split(",")(0), x.split(",")(1)))
val swapped = namePairRDD.map(item => item.swap)
val combinedOutput = swapped.combineByKey(List(_),(x:List[String], y:String) => y :: x,(x:List[String], y:List[String]) => x ::: y) 
combinedOutput.repartition(1).saveAsTextFile("spark8/result.txt")

## QN 37 Filter valid dates and save into a folder
hadoop fs -copyFromLocal spark9 /user/justycx201703

val reg1 = """(\d+)\s(\w{3})(,)\s(\d{4})""".r
val reg2 = """(\d+)(\/)(\d+)(\/)(\d{4})""".r
val reg3 = """(\d+)(-)(\d+)(-)(\d{4})""".r
val reg4 = """(\w{3})\s(\d+)(,)\s(\d{4})""".r

val feedbackRDD = sc.textFile("spark9\feedback.txt")
val feedbackSplit = feedbackRDD.map(line => line.split("|"))

val validRecords = feedbackSplit.filter(x => (reg1.pattern.matcher(x(1).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches|reg4.pattern.matcher(x(1).trim).matches))
val badRecords = feedbackSplit.filter(x => !(reg1.pattern.matcher(x(1).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches|reg4.pattern.matcher(x(1).trim).matches))

val valid = validRecords.map(e => (e(0), e(1), e(2)))
val bad = badRecords.map(e => (e(0), e(1), e(2)))

valid.repartition(1).saveAsTextFile("spark9/good.txt")
bad.repartition(1).saveAsTextFile("spark9/bad.txt")

## QN 39 Join two files
hadoop fs -copyFromLocal spark16 /user/justycx201703

val one = sc.textFile("spark16/file1.txt").map{
_.split(",",-1) match{
case Array(a,b,c) => (a,(b,c))
}
}

val two = sc.textFile("spark16/file2.txt").map{
_.split(",",-1) match{
case Array(a,b,c) => (a,(b,c))
}
}

val joined = one.join(two)
val sum = joined.map{
case(_,((_,num2),(_,_))) => num2.toInt
}.reduce(_+_)

## QN 40 
hadoop fs -copyFromLocal spark15 /user/justycx201703

val field = sc.textFile("spark15/file1.txt")
val mapper = field.map(x => x.split(",", -1))

mapper.map(x => x.map(x => {if(x.isEmpty) 0 else x})).collect
