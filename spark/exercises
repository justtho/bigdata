## QN 31 Removing the text from a file and saving it to a folder
Load the context text file:

val content = sc.textFile("Spark2/Content.txt")
val remove = sc.textFile("Spark2/Remove.txt")
val removeRDD = remove.flatMap(x => x.split(",")).map(word => word.trim)
val bRemove= sc.broadcast(removeRDD.collect().toList)
val words = content.flatMap(line => line.split(" "))
val filtered = words.filter{case(word) => !bRemove.value.contains(word)}
val pairRDD = filtered.map(word => (word,1))
val wordCount = pairRDD.reduceByKey(_+_)
wordCount.saveAsTextFile("Spark2/result.txt")

## QN 32 Filter words in a text
Load 3 text files in a folder to spark

val content = sc.textFile("Spark3/sparkdir1/file1.txt,Spark3/sparkdir2/file2.txt,Spark3/sparkdir3/file3.txt")
content.take(5).foreach(println)

val flatContent = content.flatMap(word => word.split(" "))
flatContent.take(10).foreach(println)

val trimmedContent = flatContent.map(word => word.trim)
val removeRDD = sc.parallelize(List("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of"))
val filtered = trimmedContent.subtract(removeRDD)
val pairRDD = filtered.map(word => (word,1))
val wordCount = pairRDD.reduceByKey(_+_)
val swapped = wordCount.map(item => item.swap)
val sortedOutput = swapped.sortByKey(false)
sortedOutput.saveAsTextFile("Spark3/result")
import org.apache.hadoop.io.compress.GzipCodec
sortedOutput.saveAsTextFile("Spark3/compressedresult",classOf[GzipCodec])

## QN 33 Join tables in scala
Load tables into hdfs
hadoop fs -copyFromLocal spark5 /user/justycx201703

val name = sc.textFile("spark5/EmployeeName.csv")
val namePairRDD = name.map(x =>(x.split(",")(0),x.split(",")(1)))

val salary= sc.textFile("spark5/EmployeeSalary.csv")
val salaryPairRDD = salary.map(x =>(x.split(",")(0),x.split(",")(1)))

val joined = namePairRDD.join(salaryPairRDD)
val keyRemoved = joined.values
val swapped = keyRemoved.map(item => item.swap)
val grpByKey = swapped.groupByKey.collect()
val rddByKey = grpByKey.map{case(k,v) => k->sc.makeRDD(v.toSeq)}
rddByKey.foreach{case(k,rdd) => rdd.saveAsTextFile("spark5/Employee"+k)}

## QN 34 ceating an rdd, removing headers, and filtering words
Load the data into hdfs
hadoop fs -copyFromLocal spark6 /user/justycx201703

val csv = sc.textFile("spark6/user.csv")
val headerAndRows = csv.map(line => line.split(",").map(_.trim))
val header = headerAndRows.first
val data = headerAndRows.filter(_(0)!= header(0))
val maps = data.map(splits => header.zip(splits).toMap)
#Filter out the user myself
val result = maps.filter(map => map("id") != "myself")
result.saveAsTextFile("spark6/result.txt")


